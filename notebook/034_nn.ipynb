{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "pd.set_option(\"max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR Schedulers\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr += [param_group['lr']]\n",
    "    assert(len(lr) == 1)\n",
    "    lr = lr[0]\n",
    "    return lr\n",
    "\n",
    "\n",
    "class NullScheduler():\n",
    "    def __init__(self, lr=0.01):\n",
    "        super(NullScheduler, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.cycle = 0\n",
    "\n",
    "    def __call__(self, time):\n",
    "        return self.lr\n",
    "\n",
    "    def __str__(self):\n",
    "        string = \"NullScheduler\\n\" \\\n",
    "            + \"lr={0:0.5f}\".format(self.lr)\n",
    "        return string\n",
    "\n",
    "\n",
    "class ManualScheduler():\n",
    "    def __init__(self, lr=0.01, lr_decay=0.9):\n",
    "        super(ManualScheduler, self).__init__()\n",
    "        self.lr_list = [lr * (lr_decay ** i) for i in range(100)]\n",
    "        self.cycle = 0\n",
    "\n",
    "    def __call__(self, time):\n",
    "        if time < len(self.lr_list):\n",
    "            return self.lr_list[time]\n",
    "        else:\n",
    "            return self.lr_list[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        string = \"ManualScheduler\\n\" \\\n",
    "            + \"lr={0:0.5f}\".format(self.lr_list[0])\n",
    "        return string\n",
    "\n",
    "\n",
    "class CosineAnnealingScheduler():\n",
    "    def __init__(self, eta_min=0.0001, eta_max=0.002, cycle=100, repeat=False):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.cycle = cycle\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        self.lr = self.eta_min + (self.eta_max - self.eta_min)\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        if self.repeat:\n",
    "            self.lr = self.eta_min + \\\n",
    "                (self.eta_max - self.eta_min) \\\n",
    "                * (1 + math.cos(math.pi * epoch / self.cycle)) / 2\n",
    "        else:\n",
    "            if epoch <= self.cycle:\n",
    "                self.lr = self.eta_min + \\\n",
    "                    (self.eta_max - self.eta_min) \\\n",
    "                    * (1 + math.cos(math.pi * epoch / self.cycle)) / 2\n",
    "            else:\n",
    "                self.lr = self.eta_min\n",
    "        return self.lr\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'CosineAnealingScheduler\\n' \\\n",
    "                + 'lr=%0.5f ' % (self.lr)\n",
    "        return strin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    ref: https://github.com/Bjarten/early-stopping-pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=2, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.best_model_savepath = None\n",
    "        self.best_oof_preds = None\n",
    "\n",
    "    def __call__(self, val_loss, model, save_name, oof_preds=None):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, save_name)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            # print(f'EarlyStopping counter: {self.counter} '\n",
    "            #      'out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, save_name)\n",
    "            self.best_model_savepath = save_name\n",
    "            self.counter = 0\n",
    "            if oof_preds is not None:\n",
    "                self.best_oof_preds = oof_preds\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, save_name):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ('\n",
    "                  '{self.val_loss_min:.5f} --> {val_loss:.5f}'\n",
    "                  ').  Saving model ...')\n",
    "            print(\"Save model: {}\".format(save_name))\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "        self.val_loss_min = val_loss\n",
    "    \n",
    "    def get_best_filepath(self):\n",
    "        return self.best_model_savepath\n",
    "\n",
    "    def get_best_oof_preds(self):\n",
    "        return self.best_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cache(reset=False):\n",
    "    def _feature_cache(func):\n",
    "        def wrapper(train_df, test_df, *args):\n",
    "            func_name = func.__name__\n",
    "            train_feat_path = Path(\"../feature\") / f\"train_{func_name}.pkl\"\n",
    "            test_feat_path = Path(\"../feature\") / f\"test_{func_name}.pkl\"\n",
    "            # if feature exists, load feature\n",
    "            if train_feat_path.exists() and test_feat_path.exists() and not reset:\n",
    "                train_feats = pd.read_pickle(train_feat_path).reset_index(drop=True)\n",
    "                test_feats = pd.read_pickle(test_feat_path).reset_index(drop=True)\n",
    "                train_df = pd.concat([train_df, train_feats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_feats], axis=1)\n",
    "            # if not exists, make feature and save as pickle\n",
    "            else:\n",
    "                before_cols = train_df.columns.tolist()\n",
    "                train_df, test_df = func(train_df, test_df, *args)\n",
    "                after_cols = train_df.columns.tolist()\n",
    "                new_cols = [c for c in after_cols if c not in before_cols]\n",
    "                train_feats = train_df[new_cols]\n",
    "                test_feats = test_df[new_cols]\n",
    "                train_feats.to_pickle(train_feat_path)\n",
    "                test_feats.to_pickle(test_feat_path)            \n",
    "            return train_df, test_df\n",
    "        return wrapper\n",
    "\n",
    "    return _feature_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_importance_cols(use_num=50):\n",
    "    # basicなモデルのimportanceを読み込み\n",
    "    importance_df = pd.read_csv(\"../data/importance/003_importance.csv\")\n",
    "    imp_feats = importance_df[\"feature\"].iloc[:use_num].tolist()\n",
    "    return imp_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_cat_cols(train_df):\n",
    "    tmp = train_df.iloc[:1000]\n",
    "    multi_cols = []\n",
    "    for c in train_df.columns:\n",
    "        sep_num = tmp[c].astype(str).fillna(\"\").str.contains(\";\").sum()\n",
    "        if sep_num > 10:\n",
    "            multi_cols.append(c)\n",
    "    return multi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cols = train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cat_cols = get_multi_cat_cols(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_cols = [\n",
    "    c for c in list(np.setdiff1d(original_cols, multi_cat_cols))\n",
    "    if c not in [\"Salary\", \"No\"] and \"float\" in train_df[c].dtype.name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [c for c in train_df.columns if c not in multi_cat_cols + nume_cols + [\"Salary\", \"No\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 65, 40, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_cols), len(cat_cols), len(nume_cols), len(multi_cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-category encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236ab58a67094788b47f0f02a233c4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(multi_cat_cols):\n",
    "    binarizer = MultiLabelBinarizer()\n",
    "    train_multi_srs = train_df[c].map(lambda x: x.split(\";\") if x is not np.nan else [])\n",
    "    test_multi_srs = test_df[c].map(lambda x: x.split(\";\") if x is not np.nan else [])\n",
    "    train_arr = binarizer.fit_transform(train_multi_srs)\n",
    "    test_arr = binarizer.transform(test_multi_srs)\n",
    "    feat_cols = [f\"ohe_{c}_{val}\" for val in binarizer.classes_]\n",
    "    train_feat_df = pd.DataFrame(train_arr, columns=feat_cols, dtype=np.int8)\n",
    "    test_feat_df = pd.DataFrame(test_arr, columns=feat_cols, dtype=np.int8)\n",
    "    all_feat_df = pd.concat([train_feat_df, test_feat_df], axis=0, ignore_index=True)\n",
    "    train_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "    test_df = pd.concat([test_df, test_feat_df], axis=1)\n",
    "    # ohe_featureはcategoryとnumerical両方として扱う\n",
    "    nume_cols += feat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d30efb6b9924199977d978b283a0396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_cat_cols = []\n",
    "\n",
    "imp_feats = get_basic_importance_cols(use_num=30)\n",
    "multi_new_cols = [c for c in cat_cols if c in imp_feats]\n",
    "\n",
    "for col_a, col_b in tqdm(list(itertools.combinations(multi_new_cols, 2))):\n",
    "    new_col = col_a + \"__\" + col_b\n",
    "    train_df[new_col] = train_df[col_a].astype(str) + \"__\" + train_df[col_b].astype(str)\n",
    "    test_df[new_col] = test_df[col_a].astype(str) + \"__\" + test_df[col_b].astype(str)\n",
    "    new_cat_cols.append(new_col)\n",
    "\n",
    "cat_cols += new_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97cf24d9b6c47019d8a96eedd69a239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(cat_cols):\n",
    "    train_df[c], uniques = pd.factorize(train_df[c])\n",
    "    train_df[c] += 1\n",
    "    test_df[c] = uniques.get_indexer(test_df[c]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "use_cols = [c for c in train_df.columns if c not in multi_cat_cols + [\"Salary\", \"No\"]]\n",
    "print(len(use_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[\"Salary\"] = scaler.fit_transform(train_df[\"Salary\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuevicoDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, nume_cols, mode=\"train\"):\n",
    "        print(\"Make Dataset\")\n",
    "        self.mode = mode\n",
    "        self.length = len(df)\n",
    "        ##############\n",
    "        # sales\n",
    "        ##############\n",
    "        self.df = df.copy()\n",
    "        self.cat_cols = cat_cols\n",
    "        self.nume_cols = nume_cols\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # category\n",
    "        cat_ids = self.df.iloc[idx][self.cat_cols].values.astype(int)\n",
    "        nume_x = self.df.iloc[idx][self.nume_cols].values.astype(np.float32)\n",
    "\n",
    "        target = self.df.iloc[idx][\"Salary\"] if self.mode == \"train\" else 0\n",
    "        \n",
    "        return cat_ids, nume_x, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Dataset\n"
     ]
    }
   ],
   "source": [
    "data = QuevicoDataset(train_df, cat_cols, nume_cols, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1]),\n",
       " array([ 1.,  4.,  5.,  6.,  2.,  3.,  7.,  1.,  4.,  3., 10.,  5.,  9.,\n",
       "        11.,  2.,  7.,  6.,  8.,  9.,  4.,  7.,  8.,  3.,  6., 10.,  2.,\n",
       "         1.,  5.,  1.,  2.,  4.,  3.,  5.,  1.,  2.,  6.,  3.,  5.,  7.,\n",
       "         4.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " -1.089770420705105)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, cat_cols, nume_cols, df):\n",
    "        super(NNModel, self).__init__()\n",
    "\n",
    "        n_hidden = 128\n",
    "        self.cat_cols = cat_cols\n",
    "        self.emb_dict = {}\n",
    "        total_cat_hidden = 0\n",
    "        for i, c in enumerate(cat_cols):\n",
    "            cat_nunique = df[c].max()\n",
    "            n_cat_hidden = 16 if cat_nunique > 64 else 8\n",
    "            self.emb_dict[i] = nn.Embedding(cat_nunique+10, n_cat_hidden)\n",
    "            total_cat_hidden += n_cat_hidden\n",
    "        \n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                len(nume_cols) + total_cat_hidden, 1024\n",
    "            ),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                1024, 512\n",
    "            ),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                512, 128\n",
    "            ),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                128, 1\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "       \n",
    "    def forward(self, cat_x, nume_x):\n",
    "        batch_size = cat_x.size(0)\n",
    "        # level, store_id, state_id, item_id, dept_id, cat_id, month, wday\n",
    "        emb_list = []\n",
    "        for cat_i, c in enumerate(self.cat_cols):\n",
    "            embedder = self.emb_dict[cat_i]\n",
    "            emb = embedder(cat_x[:, cat_i])\n",
    "            emb_list.append(emb)\n",
    "        # concat nume and cate embedded feature\n",
    "        concat_feature = torch.cat(emb_list, 1)\n",
    "        concat_feature = concat_feature.view([batch_size, -1])\n",
    "        concat_feature = torch.cat([concat_feature, nume_x], 1)\n",
    "        # regression layer\n",
    "        pred_y = self.reg_layer(concat_feature)\n",
    "        return pred_y\n",
    "    \n",
    "    def predict(self, cate_x, nume_x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(cate_x, nume_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_df, valid_df, cat_cols, nume_cols, debug=False):\n",
    "\n",
    "    ##################\n",
    "    # Config\n",
    "    ##################\n",
    "    config = Config()\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # dataset & dataloader\n",
    "    ##################\n",
    "    # dataset\n",
    "    train_dataset = QuevicoDataset(\n",
    "        train_df, cat_cols, nume_cols, mode=\"train\"\n",
    "    )\n",
    "    valid_dataset = QuevicoDataset(\n",
    "        valid_df, cat_cols, nume_cols, mode=\"train\"\n",
    "    )\n",
    "\n",
    "    ##################\n",
    "    # criterion\n",
    "    ##################\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    ##################\n",
    "    # archtecture\n",
    "    ##################\n",
    "    model = NNModel(cat_cols, nume_cols, train_df)\n",
    "    model.to(device)\n",
    "\n",
    "    ##################\n",
    "    # lr scheduler\n",
    "    ##################\n",
    "    scheduler = NullScheduler(lr=config.learning_rate)\n",
    "    # scheduler = ManualScheduler(lr=config.learning_rate, lr_decay=config.lr_decay)\n",
    "\n",
    "    #scheduler = CosineAnnealingScheduler(\n",
    "    #    eta_min=ca_eta_min,\n",
    "    #    eta_max=ca_eta_max,\n",
    "    #    cycle=ca_cycle,\n",
    "    #    repeat=False\n",
    "    #)\n",
    "    \n",
    "    ##################\n",
    "    # Optimiizer\n",
    "    ##################\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=scheduler(0)\n",
    "    )\n",
    "\n",
    "    ##################\n",
    "    # data loader\n",
    "    ##################\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n",
    "    # dataloaders\n",
    "    dataloaders_dict = {\n",
    "        \"train\": train_loader,\n",
    "        \"valid\": valid_loader,\n",
    "    }\n",
    "\n",
    "    ##################\n",
    "    # early stopping\n",
    "    ##################\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.n_early_stopping_patience,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    ##################\n",
    "    # train epoch loop\n",
    "    ##################\n",
    "    # iteration and loss count\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    num_epochs = config.n_epoch\n",
    "    valid_period = config.valid_period\n",
    "    val_rmse_score_list = []\n",
    "    val_wrmsse_score_list = []\n",
    "    best_oof_preds = None\n",
    "\n",
    "    print(f\"Optimizer\\n  {optimizer}\")\n",
    "    print(f\"Scheduler\\n  {scheduler}\")\n",
    "    print(f\"Batchsize\\n  {config.batch_size}\")\n",
    "    print(\"** start training here! **\")\n",
    "    print(\"                      |----- VALID ----------------|------ TRAIN -------\")\n",
    "    print(\"rate     iter   epoch |  loss  metric   wrmsse |  loss    | time  \")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        t_epoch_start = time.time()\n",
    "        val_pred_list = []\n",
    "        val_true_list = []\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                lr = scheduler(epoch)\n",
    "                if lr < 0:\n",
    "                    break\n",
    "                adjust_learning_rate(optimizer, lr)\n",
    "                model.train()\n",
    "            else:  # valid\n",
    "                if epoch % valid_period == 0:\n",
    "                    model.eval()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # get batch data loop\n",
    "            for iter_i, (cate_x, nume_x, targets) \\\n",
    "                    in enumerate(dataloaders_dict[phase]):\n",
    "                cate_x = torch.LongTensor(cate_x)\n",
    "                cate_x = cate_x.to(device)\n",
    "                nume_x = torch.FloatTensor(nume_x)\n",
    "                nume_x = nume_x.to(device)\n",
    "                targets = targets.float()\n",
    "                targets = targets.to(device)\n",
    "                # zero grad\n",
    "                optimizer.zero_grad()\n",
    "                # train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    y_pred = model(cate_x, nume_x)\n",
    "                    # calculate loss\n",
    "                    loss = criterion(\n",
    "                        y_pred.reshape(-1),\n",
    "                        targets.reshape(-1),\n",
    "                    )\n",
    "                    if phase == 'train':\n",
    "                        print(f\"\\r{iter_i*config.batch_size} / {len(train_dataset)}\", end='')\n",
    "                        loss.backward()  \n",
    "                        optimizer.step()\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "                    elif phase == \"valid\":\n",
    "                        print(f\"\\r{iter_i*config.batch_size} / {len(valid_dataset)}\", end='')\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        if epoch > 0 and (epoch % valid_period == 0):\n",
    "                            pred = model.predict(cate_x, nume_x)\n",
    "                            val_pred_list.append(pred.cpu().numpy())\n",
    "                            val_true_list.append(targets.cpu().numpy())\n",
    "\n",
    "            print(\"\", end=\"\")\n",
    "        if epoch > 0 and epoch % valid_period == 0:\n",
    "            val_preds = np.concatenate(val_pred_list, axis=0)\n",
    "            val_true = np.concatenate(val_true_list, axis=0)\n",
    "            \n",
    "            val_score = np.sqrt(mean_squared_error(\n",
    "                scaler.inverse_transform(val_true),\n",
    "                scaler.inverse_transform(val_preds)\n",
    "            ))\n",
    "\n",
    "\n",
    "        t_epoch_finish = time.time()\n",
    "        elapsed_time = t_epoch_finish - t_epoch_start\n",
    "        lr = get_learning_rate(optimizer)\n",
    "        if epoch > 0 and epoch % valid_period == 0:\n",
    "            print(f\"\\r\", end=\"\")\n",
    "            print(\n",
    "                \"{0:1.5f}  {1:4d}  {2:3d}  | {3:4.3f} {4:1.5f}  {5:1.5f}  {6:4.1f} {7:4.3f}\"\n",
    "                .format(\n",
    "                    lr,\n",
    "                    iteration,\n",
    "                    epoch,\n",
    "                    epoch_val_loss,\n",
    "                    val_score,\n",
    "                    0, # val_wrmsse_score,\n",
    "                    epoch_train_loss,\n",
    "                    elapsed_time),\n",
    "            )\n",
    "\n",
    "            ######################\n",
    "            # early stopping\n",
    "            ######################\n",
    "            if 0:\n",
    "                model_save_path = os.path.join(\n",
    "                    \"./\", f\"checkpoint_epoch{epoch}_val{val_rmse_score:.4f}.pth\"\n",
    "                )\n",
    "                early_stopping(val_rmse_score, model, model_save_path, oof_preds=denormalized_val_preds)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"******** Early stopping ********\")\n",
    "                    print(f\"Best Score: {early_stopping.best_score*(-1)}\")\n",
    "                    # load best model parameter\n",
    "                    best_model_save_path = early_stopping.get_best_filepath()\n",
    "                    model.load_state_dict(\n",
    "                        torch.load(\n",
    "                            best_model_save_path,\n",
    "                            map_location=lambda storage,\n",
    "                            loc: storage\n",
    "                        )\n",
    "                    )\n",
    "                    best_oof_preds = early_stopping.get_best_oof_preds()\n",
    "                    break\n",
    "            \n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "    return model, best_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 256\n",
    "    n_early_stopping_patience = 10\n",
    "    n_epoch = 50\n",
    "    valid_period = 1\n",
    "    learning_rate = 0.00005\n",
    "    lr_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_df, _valid_df = train_test_split(train_df, test_size=0.2, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Dataset\n",
      "Make Dataset\n",
      "Optimizer\n",
      "  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scheduler\n",
      "  NullScheduler\n",
      "lr=0.00005\n",
      "Batchsize\n",
      "  256\n",
      "** start training here! **\n",
      "                      |----- VALID ----------------|------ TRAIN -------\n",
      "rate     iter   epoch |  loss  metric   wrmsse |  loss    | time  \n",
      "-------------------------------------------------------------------------------------\n",
      "0.00005   213    1  | 7.613 23498.68750  0.00000  49.1 110.741\n",
      "0.00005   319    2  | 7.176 22812.63672  0.00000  43.1 103.628\n",
      "0.00005   425    3  | 7.070 22637.06641  0.00000  40.0 99.301\n",
      "0.00005   531    4  | 6.925 22400.08984  0.00000  38.0 99.589\n",
      "0.00005   637    5  | 6.810 22211.05078  0.00000  35.1 110.859\n",
      "0.00005   743    6  | 6.699 22020.19141  0.00000  33.4 109.200\n",
      "0.00005   849    7  | 6.678 21982.56250  0.00000  31.7 116.680\n",
      "0.00005   955    8  | 6.565 21800.01172  0.00000  30.3 102.113\n",
      "0.00005  1061    9  | 6.487 21675.68359  0.00000  29.5 103.780\n",
      "0.00005  1167   10  | 6.546 21767.73438  0.00000  28.1 125.589\n",
      "0.00005  1273   11  | 6.430 21570.89844  0.00000  27.4 109.085\n",
      "0.00005  1379   12  | 6.544 21750.96484  0.00000  26.0 113.819\n",
      "0.00005  1485   13  | 6.492 21659.96875  0.00000  25.0 114.850\n",
      "0.00005  1591   14  | 6.523 21707.67188  0.00000  24.5 111.063\n",
      "0.00005  1697   15  | 6.439 21576.72461  0.00000  23.6 111.429\n",
      "0.00005  1803   16  | 6.404 21516.63672  0.00000  22.8 110.990\n",
      "0.00005  1909   17  | 6.404 21518.92578  0.00000  22.2 113.746\n",
      "0.00005  2015   18  | 6.380 21480.98828  0.00000  21.7 102.853\n",
      "0.00005  2121   19  | 6.338 21405.31641  0.00000  20.7 91.751\n",
      "0.00005  2227   20  | 6.298 21351.58203  0.00000  20.4 93.482\n",
      "0.00005  2333   21  | 6.412 21538.22852  0.00000  19.4 94.148\n",
      "0.00005  2439   22  | 6.457 21615.97656  0.00000  19.0 93.311\n",
      "0.00005  2545   23  | 6.317 21385.30859  0.00000  18.4 92.912\n",
      "0.00005  2651   24  | 6.347 21435.66016  0.00000  17.8 105.124\n",
      "0.00005  2757   25  | 6.558 21773.31055  0.00000  16.9 96.721\n",
      "0.00005  2863   26  | 6.319 21397.36133  0.00000  16.7 94.701\n",
      "0.00005  2969   27  | 6.653 21946.43750  0.00000  16.4 95.137\n",
      "0.00005  3075   28  | 6.397 21530.86719  0.00000  16.0 96.523\n",
      "0.00005  3181   29  | 6.572 21813.10547  0.00000  15.1 100.223\n",
      "0.00005  3287   30  | 6.563 21812.65430  0.00000  14.7 98.476\n",
      "0.00005  3393   31  | 6.438 21611.49414  0.00000  14.4 91.046\n",
      "0.00005  3499   32  | 6.487 21683.11133  0.00000  14.3 91.259\n",
      "0.00005  3605   33  | 6.420 21580.52539  0.00000  13.6 95.501\n",
      "0.00005  3711   34  | 6.594 21861.77734  0.00000  13.1 90.068\n",
      "0.00005  3817   35  | 6.463 21653.26953  0.00000  13.0 89.114\n",
      "0.00005  3923   36  | 6.497 21705.85352  0.00000  12.5 84.439\n",
      "0.00005  4029   37  | 6.474 21671.64062  0.00000  12.3 84.439\n",
      "0.00005  4135   38  | 6.444 21611.29688  0.00000  12.2 84.362\n",
      "0.00005  4241   39  | 6.472 21668.63867  0.00000  11.6 84.404\n",
      "0.00005  4347   40  | 6.512 21738.85547  0.00000  11.8 84.355\n",
      "0.00005  4453   41  | 6.461 21651.52148  0.00000  11.3 83.927\n",
      "0.00005  4559   42  | 6.489 21700.11523  0.00000  11.1 83.715\n",
      "0.00005  4665   43  | 6.574 21837.04492  0.00000  10.8 84.432\n",
      "0.00005  4771   44  | 6.474 21663.64453  0.00000  10.8 83.955\n",
      "0.00005  4877   45  | 6.421 21588.20898  0.00000  10.2 84.538\n",
      "0.00005  4983   46  | 6.521 21747.71875  0.00000  10.1 84.621\n",
      "0.00005  5089   47  | 6.502 21713.07227  0.00000   9.9 83.865\n",
      "0.00005  5195   48  | 6.533 21774.81250  0.00000   9.7 83.692\n",
      "0.00005  5301   49  | 6.508 21741.66797  0.00000   9.6 83.949\n",
      "0.00005  5407   50  | 6.491 21704.62891  0.00000   9.5 83.950\n"
     ]
    }
   ],
   "source": [
    "model, oof_preds = training(_train_df, _valid_df, cat_cols, nume_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b1bac35da4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moof_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moof_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    254\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    255\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    646\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     96\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "oof_score = np.sqrt(mean_squared_error(train_df[\"Salary\"], oof_preds))\n",
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(use_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.zeros(len(test_df))\n",
    "\n",
    "for model in models:\n",
    "    test_pred += model.predict(test_df[use_cols]) / len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"../input/submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[\"Salary\"] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_df.to_csv(\"../predict/024_col_shuffle_20186.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)",
   "language": "python",
   "name": "python38264bit382pyenv0bf26b16ab884472b54c5411cc1e5c03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
